Random explanations!

Structure of operations, organization of files
----------------------------------------------
rddma_cdev and rddma_fabric deal with the file and rapidio fabric
message interfaces. The receive and send strings of the following
style:

   op_name://name.location#offset:extent?var=val,var=val

The both call the common do_operation after taking care of any of the
house work specific to their interfaces to handle the operations of
the interfaces, asynchronicity, call-backs, etc, etc.

rddma_ops.[ch] contains do_operation and the abstract operations which
convert the strings above into function calls of the operators in
question

op_name://desc ---> struct op_data *op_func(loc, desc)

etc.

In general the op_func for op_name on desc is found in the
ops of the location component. So the top level abstract operations in
rddma_ops.[ch] simply parse the string, find the location and indirect
via its ops.

There are generic functions for dealing with the strings, parsing,
printing them canonically etc in rddma_parse.[ch]

The abstract types we deal with and their hierarchy are:
rddma subsystem kset of location 
    location kobj ->smbs ->xrefs
	smbs kset smb
		smb kobj
	xfers kset xfer
		xfer kobj -> binds
			binds kset bind
                           bind kobj -> dsts
				dsts kset dst
					dst kobj -> srcs
						srcs kset src
							src kobj

The location is the fundamental type from which the op_func types are
derived. There are two fundamental types of operations, local and
fabric.

local operations generate data structures locally.
fabric operations generate data structures remotely together with a
corresponding local structure to track the remote partner.

So the operations for an smb and xfer are inherited from their
location, bind from xfer, dst from bind, src from dst.

These two classes of operations (just bandying terms to confuse the
C++ guys :-) are found in rddma_local_ops.[ch] and rddma_fabric.[ch]

Each of the fundamental types is represented by the files
rddma_<type>.[ch], e.g rddma_location.[ch], rddma_smb.[ch], etc.

These files define a data structure and its methods. These are heavily
based on kobject/kset code for refcounting, lifecycle, etc., so in
fact the base of these files are generated code from a shell script.

Some of the relationships are one to many, eg an xfer may have many
bind mappings defined on it. These are represented
with ksets, so in addition to rddma_smb.[ch], etc you will also see
rddma_smbs.[ch] for the set operations. These are mainly boilerplate
generated code necessary to make the hierarchy work with one to many
relationships so you can ignore these files.

Each type file will include operations for generating a new instance
of the type and registering it with the subsystem.
   new_rddma_<type>
   rddma_<type>_register
   rddma_<type>_unregister

These are also combined in create and delete methods
      rddma_<type>_create
      rddma_<type>_delete

These methods should not be confused with the fabric and local methods
of similar name and function. These are concered simply with the
kobject/sysfs aspects of managing the data structures on the local
machine. These operations are used by both the fabric and local
op_functions. These in turn should not be confused with the highest
level abstract operators of do_operation.

The hierarchy looks like:

do_operation("op://str")
	op_func(loc,str)
		op_<local|fabric>_func(loc,str)
			rddma_<type>_funcs(...)

In addition to the basic create/delete fucntions there are find
functions corresponding to name lookups. Name creation/deletion is
automatic as a by-product of the type create/deletes and the service
is distributed as required by virtue of the location ops. Again there
is a hierarchy of find operations from the do_operation level on a
descriptor string at the top to the basic kset/kobject types at the
bottom.

Finally, there are two operations on xfers, start and stop.

Nobody expects the spanish inquisition: there's also the map operators
for mapping SMBs into user space and between locations using the
LAW/ATMU mappings of the soc.

Thats it folks!

Everything else is just the glue to make it all work.

Fabric Subsystem
----------------

The fabric bottom layer of the rddma driver is in rddma_fabric.[ch] It
publishes a registration function and an rddma_fabric_address
structure with some operations which a "fabric transport" module must
adhere to. A "fabric transport" is what actually does the work of
moving messages over some media. In most final real products this will
be the rapid io messaging system.

As a first worked example I am creating rddma_fabric_net.c as. This
module implements an ethernet framed protocol exchange over some
network media. What this media is is not fixed by this transport
because it uses the dev_add_pack and a named network device to
capture/insert frames with a particular ethertype (also a parameter of
the module). 

Thus you could use this to run over ethernet, in our early test
harness approaches this could indeed be real ethernet between our test
machines, laptops, workstations, etc., in the real product it could be
a backplane 10Gig ethernet and/it can include rionet, the network
stack module for rapidio backplane.

If you use the linux bridge code you can in fact attach multiple
interfaces, eth0, eth1, rionet0, etc., to a bridge, br0 for example,
and then run the fabric transport over the bridge.

If you use the vlan software on the interfaces you can run multiple
logical bridges, br0, br1, etc., and run secure isolated virtual
networks and have a rddma transport in each. Then you can build
locations in separte networks. Good spiel for marketing to "security"
(not mil grade unless you invoke Selinux).

As well as working out the registration handshaking for a transport
and the rddma driver this is also forcing me to work out the
dns,arp,ip etc., analogues to my location system.

Essentially I'm using the location offset as a rddma "IP address".

So an rddma_location to rddma_fabric_address binding is a DNS
resolution (name to IP address) and the rddma to transport binding is
an "IP address" to transport "mac address" arp binding.

The way this interacts with a remote configurator setting up the
locations and their bindings is effectively a bootp/dhcp mechanism.

I'm only talking analogues in the above. Our driver isn't a
replacement for IP!

DMA Subsystem
-------------
Oh, still to come, rddma_dma.[ch] the interface to the dma
subsystem. Again this is glue like rddma_cdev and rddma_fabric to glue
the subsystem to the DMA device.


Rincon Picture
--------------
	          userland procs
			|
			v
		    rddma_cdev
			|
			v
	          rddma subsystem
		     /      \
		    /        \
		   /          \
	     rddma_dma      rddma_fabric
		/ |          |     \
	       /  |          |      \
    dma hardware  |          |   rio_hardware
		  |          |
             pseduo hw    other fabrics


Creating Binds and Xfers
------------------------

An SMB has a name and location (fqn if you like) with attributes size.
   smb.board.fabric:size

So a bind is
   smbdest.boardx.fabric#dstoffset:extent=smbsrc.boardy.fabric#srcoffset:extent

the half binds are the two smb#offset:extents either side of the equal
sign, eg dst=src. A feature of a bind is that the src:extent and the
dst:extent must be equal. If you leave one out it will default to the
value of the other. If you leave both out it will default to the
extent you specify in the xfer, e.g.,
       xfer.location:extent/dst=src

If you leave them all out then it will default to the minimum of the
rest of the dst and src.

So when the above bind is given in a xfer_create we find the two smbs
check the offset extent is in range and then create the dst and src in
the dsts and srcs sets of the respective smbs.

Well almost...

It does not make sense to have multiple equal or overlapping dst
components on an SMB. So if an dst has attributes offset and extent,
d->o and d->e then if someone wants to a new dst, y#o:e, then
for all x in smb.dsts

    x->o > y->o + y->e || y->o > x->o + x->e

so when given a bind in an xfer we validate
   1. the smbs exist,
   2. that the offset and extent is in range of the smb,
   3. that both extents are equal(or can be made so if defaulting),
   4. that the dst is unique in dst->dsts

then we create dst in dsts and src in srcs and create bind in xfer

So an xfer is a set of binds, each bind has an offset and extent.
   d->e == s->e == b->e

What is the offset of a bind and what is the size of an xfer?

Basically, a xfer is the dma'ing of a lot of binds, the amount of data
transferred is the sum of the extents of all the binds in the xfer.
	    size = sum(b->e)
so one definition of the extent of an xfer could be its size.

The binds are created in some order and we can arbitrarily define the
offset of the bind as the position in the xfer. If you don't specify
an offset for the bind it defaults to the current extent of the xfer,
i.e., its added to the end. However the user could create the binds
and specify their order in the dma chain by specifying the offset and
extent. This means that an xfer can have holes in it. Because the binds only
have a notional offset, extent in the xfer we don't really mind
duplicates other than that we need unique names for the binds in the
xfer set. So for all b in an xfer set the duple b->o, b->e is unique,
xfer->e is the max of b->o + b->e for all b in xfer and x->size is the
sum of b->e for all b in x.

This is neat. If you specify xfer:e/dst#o=src#o for all your binds as
you create them then the xfer naturally builds into a packed set of
binds where x->e == x->size and each bind has a unique offset. This is
the most natural way of specifying everything. But if at some point
later you want to add a new bind to it and you need it to occurr
earlier than at the end you can simply specify
	xfer#o:e/dst#o=src#o
and it will work as expected except now x->size > x->e and theres an
"overlapping" bind in there somewhere.

Sub-binds
---------

When a user specifies a half-bind in terms of arbitrary virtual
offsets and extents it has to be translated by the rddma into a set of
real DMA transactions dealing in physical addresses which are page
size limited.

If the #o:e is page aligned and a clean multiple of page size then it will
turn into :e/PAGE_SIZE DMA transactions.

If it is page aligned but not a clean multile of page size then it will turn
into :e/PAGE_SIZE + 1 DMA transactions.

If it is a clean multiple but not aligned then it will turn into
:e/PAGE_SIZE + 2 DMAs

If it is neither clean nor aligned then its from 2 * (:e/PAGE_SIZE) +
1 to 2 * (:e/PAGE_SIZE) + 1 DMAs depending on the alignment of the
end.

#define CLEAN_START(b) (((b)->o & PAGE_MASK) ? 1 : 0)
#define CLEAN_END(b) ((((b)->o + (b)->e) & PAGE_MASK) ? 1 : 0)
#define CLEAN_SHIFT(b) (CLEAN_START((b)) ? 0 : 1)
#define NUM_DMA(b) ((((b)->e / PAGE_SIZE) << CLEAN_SHIFT((b))) \
                    + CLEAN_START((b)) + CLEAN_END((b)))

So far so good. But...

Even if my half of the bind is clean the other half may be non-clean
so I'll still have to create lots of DMAs. So its a co-operative
effort to translate a bind dst#o:e=src#o:e into a set of sub-binds
representing the actual DMA transactions.

So... you can't acutally create the half-bind or bind that the user
designates, you can only create that as place holder parent for the
set of actual sub-binds which get created by requesting your nominated
sub-bind. Lets run this from the dest side, ie dest proposes, src
disposes.

With a real number example:

     User proposed bind: xfer:2000/dst#4000=src#c000

Note: My offset to the src is an offset to his named smb and is
notionally a clean start. However I have no idea what the real offset
is if src can be created on non-aligned start. So it may be clean or
it may not.

In this example I'm using a clean size, ie two 4K pages.

So dst proposes its first sub-bind:
   dst#4000:2000/dst#xxxxx000:1000=src#c000:1000

If the src is aligned it will respond by creating
   dst#4000:2000/dst#xxxxx000:1000=src#yyyyy000:1000

which is a null operation indicating agreement with my proposed
sub-bind.

However, it might reply instead with two transactions (can only be two
cos my initial proposals are always clean size)
    dst#4000:2000/dst#xxxxx000:800=src#yyyyy000:800
    dst#4000:2000/dst#xxxxx800:800=src#yyyyy800:800

ie its split my proposal in two. It will always be two but not
necessarily half as shown above. The division may be anywhere with the
1000 page size.

In this case I adjust the extent of my proposed sub-bind to match the
src's proposal and create the second sub-bind as requested.

OK, that's the flow, now lets look at some details of the above and
consider the sysfs structure which results.

Lets assume we are in a chassis with a fabric so the base location on
all boards is a say, rapido fabric called "fabric".

Each board has its own location name on the fabric, this could be the
old fashioned slot1, slot2, etc. but should be the more abstract and
therefore useful, radar1, proc1, etc. So lets assume that src is an
smb on an input IO board called radar.fabric, eg. srcsmb.radar.fabric
and that dst is an smb on a processor board called proc.fabric, eg,
dstsmb.proc.fabric.

Worked Example
--------------

So just for grins lets assume that fred asks dma to perform the xfer
between proc and radar, ie four boards are involved, fred the
initiator of these requests:

     smb_create://srcsmb.radar.fabric#800:10000
     smb_create://dstsmb.proc.fabric:8000
     xfer_create://toproc.dma.fabric:2000/dstsmb.proc.fabric#4000=srcsmb.radar.fabric#c000
     xfer_start://toproc.dma.fabric

dma the board which will execute the xfer_create and proc and radar
the boards with the destination and source smbs respectively.

Initially, each boards sysfs would look as follows:

   /sys/rddma/
		fabric/
				smbs/
				xfers/
		xxx.fabric/
				smbs/
				xfers/

where xxx is each boards respective name, fred, dma, proc, and
radar. 

After executing the two smb_creates the situation for fred would be:

   /sys/rddma/
		fabric/
				smbs/
				xfers/
		fred.fabric/
				smbs/
				xfers/
		proc.fabric/
				smbs/
					dstsmb/ 
						size <- file containing 8000
						offset <- file containing 0
				xfers/
		radar.fabric/
				smbs/
					srcsmb/ 
						size <- file containing 10000
						offset <- file containing 800
				xfers/

Notice a lot of things about the above. Fred has smb structure data
even though the smb's are remote from him. This is indicated by the
smb's being "filed" under their respective location directories. The
location directories got created because fred needed to communicate to
some guy "radar" and some guy "proc" when processing the respecitve
smb create requests. 

The flow went as follows:

   /sys/rddma/
		fabric/
				smbs/
				xfers/
		fred.fabric/
				smbs/
				xfers/
		radar.fabric/
				smbs/
					srcsmb/
						size <- file containing 10000
						offset <- file containing 800
				xfers/

For proc:

   /sys/rddma/
		fabric/
				smbs/
				xfers/
		fred.fabric/
				smbs/
				xfers/
		proc.fabric/
				smbs/
					dstsmb/ 
						size <- file containing 8000
						offset <- file containing 0
				xfers/
and dma:

   /sys/rddma/
		fabric/
				smbs/
				xfers/
		dma.fabric/
				smbs/
				xfers/

Each board shows locations for other boards it has interacted with in
addition to itself. Fred shows smbs at two of those "remote" locations
because it knows it created them. Those remote locations have those
smbs showing at their respective "local" location but are unaware of
each other, theres been no interactin between src and dst.

dma has had no interactions and just sits with himself doing nothing.

The interaction were:

    fred->radar:
	smb_create://srcsmb.radar.fabric#800:10000

    fred->proc:
	smb_create://dstsmb.proc.fabric:8000

Now lets follow fred -> dma with
    xfer_create://toproc.dma.fabric:2000/dstsmb.proc.fabric#4000=srcsmb.radar.fabric#c000

here is the set of transaction across the fabric which will results
from dma executing the above. To try and reduce the fluff I've
abbreviated all the names.

fred -> dma
	xfer_create://tp.x:2000/d.p#4000=s.r#c000

 dma -> proc
	 dsts_create://tp.x:2000/d.p#4000=s.r#c000

  proc -> dma
	  dst_create://tp.x:2000/d.p#uuuuu000:1000=s.r#c000:1000

   dma -> radar
	   srcs_create://tp.x:2000/d.p#uuuuu000:1000=s.r#c000:1000

    radar -> dma
	    src_create://tp.x:2000/d.p#uuuuu000:800=s.r#xxxxx800:800
	    src_create://tp.x:2000/d.p#uuuuu800:800=s.r#yyyyy000:800

  proc -> dma
	  dst-create://tp.x:2000/d.p#vvvvv000:1000=s.r#d000:1000

   dma -> radar
	   srcs_create://tp.x:2000/d.p#vvvvv000:1000=s.r#d000:1000

    radar -> dma
	    src_create://tp.x:2000/d.p#vvvvv000:800=s.r#yyyyy800:800
	    src_create://tp.x:2000/d.p#vvvvv800:800=s.r#zzzzz000:800

The indentation shows the call tree.

Lets look at the resultant sysfs structure on each board.

Fred will simply create the location and xfers to show he requested
it:
	/sys/rddma/
		fabric/	
				smbs/
				xfers/
		fred.fabric/
				smbs/
				xfers/
		proc.fabric/
				smbs/
					dstsmb/ 
				xfers/
		radar.fabric/
				smbs/
					srcsmb/ 
				xfers/
		dma.fabric/
				smbs/
				xfers/
					toproc/
                                             binds/
					        toproc.dma.fabric:2000
							dstsmb.proc.fabric#4000=srcsmb.radar.fabric#c000

For proc the result is:
    /sys/rddma/
	fabric/
		smbs/
		xfers/
	proc.fabric/
		smbs/
			dstsmb/
		xfers/
	dma.fabric/
		smbs/
		xfers/
			toproc/
				binds/
					toproc.dma.fabric:2000/
						dstsmb.proc.fabric#4000=srcsmb.radar.fabric#c000/
							dstsmb.proc.fabric#uuuuu000:1000/
								srcsmb.radar.fabric#c000:1000/
							dstsmb.proc.fabric#vvvvv000:1000/
								srcsmb.radar.fabric#d000:1000/
For radar the result is:
    /sys/rddma/
	fabric/
		smbs/
		xfers/
	fred.fabric/
		smbs/
		xfers/
	dma.fabric/
		smbs/
		xfers/
			toproc/
				binds/
					toproc.dma.fabric:2000
						dsts/
							dstsmb.proc.fabric#uuuuu000:1000
								srcsmb.radar.fabric#c000:1000/
									srcsmb.radar.fabric#xxxxx800:800
									srcsmb.radar.fabric#yyyyy000:800
							dstsmb.proc.fabric#vvvvv000:1000
								srcsbm.radar.fabric#d000:1000/
									srcsmb.radar.fabric#yyyyy800:800
									srcsmb.radar.fabric#zzzzz000:800


For dma the result is:

    /sys/rddma/
	fabric/
		smbs/
		xfers/
	fred.fabric/
		smbs/
		xfers/
	dma.fabric/
		smbs/
		xfers/
			toproc/
				binds/
					toproc.dma.fabric:2000
						dstsmb.proc.fabric#4000=srcsmb.radar.fabric#c000/
							dstsmb.proc.fabric#uuuuu000:1000
								srcsmb.radar.fabric#c000:1000/
									srcsmb.radar.fabric#xxxxx800:800
									srcsmb.radar.fabric#yyyyy000:800
							dstsmb.proc.fabric#vvvvv000:1000
								srcsmb.radar.fabric#d000:1000/
									srcsmb.radar.fabric#yyyyy800:800
									srcsmb.radar.fabric#zzzzz000:800
fred saw
	xfer_create://tp.x:2000/d.p#4000=s.r#c000

proc saw
	dsts_create://tp.x:2000/d.p#4000=s.r#c000

radar saw
	srcs_create://tp.x:2000/d.p#uuuuu000:1000=s.r#c000:1000
	srcs_create://tp.x:2000/d.p#vvvvv000:1000=s.r#d000:1000

dma saw
	xfer_create://tp.x:2000/d.p#4000=s.r#c000
	dst_create://tp.x:2000/d.p#uuuuu000:1000=s.r#c000:1000
	src_create://tp.x:2000/d.p#uuuuu000:800=s.r#xxxxx800:800
	src_create://tp.x:2000/d.p#uuuuu800:800=s.r#yyyyy000:800
	dst-create://tp.x:2000/d.p#vvvvv000:1000=s.r#d000:1000
	src_create://tp.x:2000/d.p#vvvvv000:800=s.r#yyyyy800:800
	src_create://tp.x:2000/d.p#vvvvv800:800=s.r#zzzzz000:800

The only disparity we can observer here is that radar, the source
board, doesn't know what to call his dsts structure, as we can see
from the who saw what list above, he's the only one who can't see the
d.p#4000=s.r#c000 detail: so he simply calls the structure for sysfs
purposes, "dsts". This structure is accessed via a pointer from the
parent structure rather than being found in a set so its name doesn't
really matter. We could call it "dsts" for all of the actors above
with no change in function but there would be a loss for us as humans
observing the sys structure for debug/information. We would not be able
to know what the value of the virtual offsets in the original requests
from the application.

Name Server, DNS, ARP, etc
--------------------------

The "location" is an abstraction of "networking" where the network may
be a real network of ethernet backplane, or a pseudo network of a
backplane fabric, etc.

Locations are "named" entities.

Names must be translated into "addresses" just as DNS translates a
name to an IP address. The "network" address should be abstract and
distinct from the media specific addressing, again just as in IP.

For this we have created the notion of the rddma_fabric_address type.

These abstract addresses must then be translated to the media specific
addresses of the particular backplane transport media, RapidIO,
PCI-Express, GbE, etc. This is an arp like process.

Currently we are working with two concrete media types in the rddma
driver as exemplar's. The Serial RapidIO fabric, where the media
address is the 8-bit RapidIO node address and an ethernet frame based
protocol where the media address is the 48-bit MAC address.

At the "IP" level I am using a 32-bit "identifier" as the
rddma_fabric_address type.

I am then overloading the extent/offset notation to describe source
and destination identifiers in the use of named locations.

	me.fabric:13

would be a way of specifying that the location name, me.fabric,
has a source identity of 13.

	someboard.fabric#27

would be a way of specifying that the destination address to use for
someboard.fabric is 27.

	someboard.fabric#27:13

would be an entry on me.fabric specifying that someboard.fabric should
be addressed by a frame with a destiantion identifier of 27 and a
source identifier of 13, me.

When  you create a sysfs tree of location entries with the above
attributes of extents and offsets you are really creating a DNS
cache. You are also as a by-product creating a routing table,
specifying default routers, default nameservers, etc.

location_create is used to configure things, ie you are configuring
the default route, the default nameserver, your own address, some
remote boards address, etc.

location_find is used to discover such configurations, to look them up
and cache the results locally, ie is the name server functionality.

So lets just walk thru some usage scenario's to see the kind of things
we are trying to achieve, and then we can design how this will be
implemented.

So I'm some freshly powered up node with an empty /sys/rddma tree. By
definition at this point I don't know my own address, I don't know
what network I'm attached to or indeed if I am even attached to any
networks, I don't know who to ask for information, don't know the name
or address of the nameserver, the network, etc.

Clearly, we cannot within the rddma driver know which networks we
should connect to in order to peform out function so this must be
configured by means extraneous. The rddma_fabric_net or
rddma_fabric_rio modules must be inserted and registered with the
driver and these must be configured with the right interfaces, fabric
addresses, etc. The assignment of MAC addresses, fabric enumeration,
etc., are all outside of the scope of the RDDMA driver.

so lets assume that rddma_fabric_net has been inserted with a network
device of either eth0, br0, rionet0, etc, and we have a MAC address.

For real ethernet drivers, the mac address is some 48-bit number.

For rionet I'm going to assume that its some base address modified
with the fabrics enumerated 8-bit device id.

We are not going to assume that the mac address allows us to identify
the system controller, eg xx:xx:xx:xx:xx:00, nor that the system
controller is necessarily the root nameserver, etc.

Instead we are going to control these aspects via conventional use of
the network address, the 32-bit source and destination identifiers,
expressed as the offset and extent attributes of named locations.

So with no more than the above information how would we expect
	location_find://fabric

to behave?

1. Our parsing rules tell us that this is shorthand for 
	location_find://fabric#0:0

so we can assume that network addresses of 0 are "unknown" values. The
above is saying, "I'm trying to get information on a network called
fabric and I don't know who to ask (#0) and I don't know who I am
(:0)"

Our implementation should, if we are to get anywhere, broadcast the
above request on the network device. The frame/packet sent out will
therefore be

	ffffffffffff xxxxxxxxxxxx 0 0 location_find://fabric#0:0

The first two fields are the MAC destination and source fields
(destination always given first). The next two are the destination and
source network identifiers (both zero)  and the rest is the string command.

I am eliding fields such as the ethertype, CRC, etc., specific to any
media type.

The source mac will be a known defined value. 

Who will respond to such a frame?

Somewhere someone must have been ab initio configured to be the root
nameserver. Somewhere, someone executed

	location_create://fabric:n

Their extent defines their source network address as n.

So when they reply to the request we want the result to be that the
location find result is the creation of a sysfs entry on the requestor
of:

	fabric#n

NB, the find result is an *offset* n and an extent of 0. In other
words our requstor now knows the destination address to use to address
the "fabric" controller but still doesn't know what its own address
is. This implies that the implementation will involve swapping
offset/extent field during a fabric exchange, just as source and
destiantion addresses are "swapped" in a request/reply frame/packet
exchanges in any network stack.

Before we continue to look at how we can continue to evolve the above
system at the name/address level, lets just digress to look at some
lower level address/mac issues.

When frames come in from a fabric/network we can "snoop" their headers
to build up caches of network address to mac address correspondences,
the ARP table. So in the above example everyone has seen a frame from
xxxxxxxxxxxx broadcast around. In this particular example the source
is saying it doens't know its network address (0) so no-one can
actually learn or cache anything above the fact that a new node with a
mac address of xxxxxxxxxxxx is active. But in what follows assume that
everyone on receiving a frame is updating a cache of mac to address
tables indexed by address. This table can be used to address frames to
nodes instead of broadcasting to unknowns. For replies the source
address in the request can be used, so in this example, no table entry
exists for xxxxxxxxxxxx when its reply is sent because the network
address is the unknown (0) value.

We snoop also frames being sent so if a reply or request to
xxxxxxxxxxxx did have a destination network address it could be
snooped into the table for subsequent use.

So to return from the dirgression and look at the evolution of the
nameserver/routing tables.

There are a number of ways we could evolve the system all of which
have analogues in the IP networking world.

1. Distributed Peers

Everyone can be responsible for configuring their own local name,
address and mac and knows ex machina ab inito the name and/or address
of their nameserver, default route, etc.

2. Centralized Server

A central server is configured with everyones information or generates
such information on demand from a pool of available resources. The
bootp/dhcp approach. Nodes may be configured with some/all/no
additional information to allow the server to allocate the
remainder. Thus a node may have a hostname (someboard) and be issued
its fully qualifed name (hostname plus domainname, in our case
someboard.fabric) plus its address, or it may just have its source mac
address and the central server issues it its hostname and address.

3. Centralized Master 

The central board instead of replying to node requests in the client
server mode can be programmed to explicitly configure the boards
remotely with the individual nodes being quiescent and passive until
spoken to.

Lets look at these in turn to see how they are implemented in terms of
our location operators.

For a system of self-configuring peers a node can simply
	
	location_create://fabric#1:13
	location_create://me.fabric?default_ops=private

my hostname is me, my network name is fabric, the fabric
controller/name server is at address 1 and my address is 13.

If it now executes 

	location_find://you.fabric

address 1 will be asked for information about you.fabric.

How will 1 know about you, me and the other nodes?

Clearly, we would like the location_create://me.fabric above to notify
the central fabric controller of this event, ie the publication of the
name "me". This will occur because location_create://fabric#1:13 has
default_ops of public, and location_create://me.fabric will have
inherited #1:13 from fabric and will execute the location_create via
fabric. Hence 1 will do a local create of me.fabric#13:1. Similarly,
it will do a creation of you.fabric#17:1 when you performs its fabric
based create.

General Principles: The extent in all locations is the same and
represents the source hosting it. The offset represents the address of
the object named. On each node there will be at least one location
where offset == extent and represents the node itself. If some one
does a broadcast (dest id == unknown) find of a location clearly every
location that has been in touch with that location is capable of
replying with the address. Only one however satisfies offset==extent
and it should provide the definitive answer. If some one does a
unicast (dest id == n) find then only the addressed location should
reply, with the address if it has it otherwise with unknown.


	A node					Syscon
					location_create://fabric#1:1
					location_create://node.fabric#4:1
location_find://fabric
	fabric#1:0

location_find://node.fabric
	node.fabric#4:4

Synchronization Objects
=======================

sync_create://ok.master.fabric#8

creates a sync object called ok at master.fabric with an inital count of 8.

Any one performing sync_wait://ok.master.fabric will block until at
least eight people have performed sync_send://ok.master.fabric at
which point all such blocked waiters will be unblocked.

Wait blocks unless the count is zero. send decrements the count.
create initializes the count.

To reuse a sync object sync_wait://ok.master.fabrc#8 will set the
count to eight if and only if it is currently zero and will then wait
on it.

If someone wants to re-initialize the count but not themselves block
on wait they can similarly used sync_send://ok.master.fabric#8 which
will set the count to 8 if and only if it is currently zero and will
then immediately perform the normal send semantic and decrement it to
7.

